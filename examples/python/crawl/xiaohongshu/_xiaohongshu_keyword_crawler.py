import asyncio
import os
import logging
from typing import Self
from urllib.parse import urlparse
from random import randint

import httpx
import aiofiles
from crawl4ai import DefaultMarkdownGenerator
from playwright.async_api import Page, Locator

from crawl.utils.time_utils import parse_publish_time, rand_swing, rand_swing_ms
from crawl.models.crawl_page import CrawlPageBase, ImageInfo
from crawl.models.crawl_model import CrawlSite, CrawlTaskParam

from ._helper import normalize_url, extract_note_id

logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
EXPLORE_URL = 'https://www.xiaohongshu.com/explore'


class XiaohongshuKeywordCrawler:
  """å°çº¢ä¹¦çˆ¬è™«å…³é”®è¯"""

  def __init__(self, site: CrawlSite, crawl_param: CrawlTaskParam, page: Page):
    self.site = site
    self.crawl_param = crawl_param
    self.page = page
    self.notes: list[CrawlPageBase] = []
    self.pending_note_items: list[Locator] = []
    self.index = 0

  async def astart(self) -> Self:
    max_notes = self.crawl_param.max_articles or self.site.max_articles
    logger.info(f'ğŸš€ å¼€å§‹çˆ¬å–å°çº¢ä¹¦ç¬”è®°ï¼Œå…³é”®è¯: {self.crawl_param.keyword}ï¼Œæœ€å¤§æ•°é‡: {max_notes}')

    # 1. å¯¼èˆªåˆ°æ¢ç´¢é¡µé¢
    await self.navigate_to_explore()

    await self.check_login()  # è‹¥æœªç™»å½•ï¼Œå°†æŠ›å‡ºå¼‚å¸¸

    # 2. æœç´¢ç¬”è®°
    search_success = await self.search_notes(self.crawl_param.keyword)
    if not search_success:
      return self

    # 3. è·å–ç¬”è®°åˆ—è¡¨
    note_items = await self.get_note_items()
    if not note_items:
      logger.warning('æœªæ‰¾åˆ°ä»»ä½•ç¬”è®°')
      return self

    # 4. é™åˆ¶å¤„ç†æ•°é‡
    self.pending_note_items = note_items[:max_notes]
    self.pending_note_items.reverse()
    logger.info(f'å‡†å¤‡å¤„ç† {len(self.pending_note_items)} ä¸ªç¬”è®°')

    return self

  async def aclose(self):
    if not self.page.is_closed():
      await self.page.close()

  def has_next(self) -> bool:
    return self.pending_note_items

  async def run_next(self) -> CrawlPageBase | None:
    if not self.has_next():
      return None

    self.index += 1
    note_item = self.pending_note_items.pop()

    # ç‚¹å‡» ESC é”®ï¼Œç¡®ä¿å½“è¯¦æƒ…å¼¹çª—æœªå…³é—­æ—¶å°†å…¶å…³é—­
    await self.page.keyboard.press('Escape')

    crawl_page = await self.extract_note_detail(note_item, self.index)
    if crawl_page:
      await self.save_note(crawl_page)
      self.notes.append(crawl_page)

    # å…³é—­è¯¦æƒ…å¼¹çª—ï¼ˆç‚¹å‡»é®ç½©åŒºåŸŸï¼‰
    await self._close_modal(escape=True)

  async def _close_modal(self, escape: bool = False):
    await self.page.wait_for_timeout(rand_swing(1))
    close_btn = self.page.locator('.close-circle')
    if await close_btn.count() > 0 and await close_btn.is_visible():
      await close_btn.click()
      logger.info('å…³é—­è¯¦æƒ…å¼¹çª—æˆåŠŸ')
    elif escape:
      await self.page.keyboard.press('Escape')
      logger.info('å…³é—­è¯¦æƒ…å¼¹çª—æˆåŠŸï¼Œä½¿ç”¨ Escape')

  async def navigate_to_explore(self):
    """å¯¼èˆªåˆ°å°çº¢ä¹¦æ¢ç´¢é¡µé¢"""
    logger.info(f'å¯¼èˆªåˆ°æ¢ç´¢é¡µé¢: {EXPLORE_URL}')
    await self.page.goto(EXPLORE_URL, wait_until='domcontentloaded')
    await self.page.wait_for_timeout(3000)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½

  async def check_login(self):
    """æ£€æŸ¥æ˜¯å¦ç™»å½•"""
    login_container = self.page.locator('.login-container')
    login_btn = login_container.get_by_role('button', name='ç™»å½•')
    if await login_btn.is_visible():
      raise Exception('æœªç™»å½•ï¼Œéœ€è¦ç™»å½•è®¿é—®')

  async def search_notes(self, keyword: str) -> bool:
    """æœç´¢ç¬”è®°"""

    try:
      # 1. å®šä½æœç´¢æ¡†å¹¶è¾“å…¥å…³é”®è¯
      search_input = self.page.locator('.input-box #search-input')
      await search_input.wait_for(state='visible', timeout=10000)
      await search_input.click()
      await search_input.fill(keyword)

      # 2. ç‚¹å‡»æœç´¢æŒ‰é’®
      search_btn = self.page.locator('.input-box .search-icon')
      await search_btn.click()
      await self.page.wait_for_timeout(rand_swing_ms(2000))

      # 3. ç‚¹å‡»å›¾æ–‡æŒ‰é’®ï¼Œåªæ˜¾ç¤ºå›¾æ–‡ç›¸å…³ç¬”è®°
      content_image_btn = self.page.locator('#image')
      await content_image_btn.wait_for(state='visible', timeout=10000)
      await content_image_btn.click()
      await self.page.wait_for_timeout(rand_swing_ms(500))

      # # 4. ç‚¹å‡»ç­›é€‰ï¼ŒæŒ‰æ—¶é—´æ’åº
      # await self.page.locator('.filter', has_text='ç­›é€‰').hover()
      # await self.page.get_by_text(re.compile('^ç­›é€‰$')).hover()
      # await self.page.wait_for_timeout(rand_swing_ms(500))
      # await self.page.locator('.filters span', has_text='æœ€æ–°').click()
      # await self.page.wait_for_timeout(rand_swing_ms(1000))

      logger.info('æœç´¢å®Œæˆ')
      return True

    except Exception as e:
      logger.error(f'æœç´¢å¤±è´¥: {e}')
      return False

  async def get_note_items(self) -> list[Locator]:
    """è·å–ç¬¬ä¸€å±çš„æ‰€æœ‰ç¬”è®°é¡¹"""
    logger.info('è·å–ç¬”è®°åˆ—è¡¨...')
    locators = []

    try:
      # ç­‰å¾…ç¬”è®°å®¹å™¨å‡ºç°
      await self.page.wait_for_selector('.feeds-container .note-item', timeout=10000)
      note_items_locator = self.page.locator('.feeds-container .note-item')

      # è·å–å¯è§çš„ç¬”è®°é¡¹æ•°é‡
      count = await note_items_locator.count()
      logger.info(f'æ‰¾åˆ° {count} ä¸ªç¬”è®°')

      for i in range(count):
        locator = note_items_locator.nth(i)
        if await locator.locator('.query-note-list').is_visible():  # è¿‡æ»¤æ‰æœç´¢é¡¹
          continue
        locators.append(locator)

    except Exception as e:
      logger.error(f'è·å–ç¬”è®°åˆ—è¡¨å¤±è´¥: {e}')

    return locators

  async def extract_note_detail(self, note_item_locator: Locator, index: int) -> CrawlPageBase | None:
    """æå–å•ä¸ªç¬”è®°çš„è¯¦ç»†ä¿¡æ¯"""
    logger.info(f'æ­£åœ¨æå–ç¬”è®° index: {index}, url: {self.page.url}')

    try:
      # 1. ç‚¹å‡»ç¬”è®°å›¾ç‰‡æ‰“å¼€è¯¦æƒ…
      img_link = None
      for selector in ['a.title', 'a.cover']:
        link = note_item_locator.locator(selector)
        if await link.count() > 0:
          img_link = link
          logger.info(f'æ‰¾åˆ°ç¬”è®°å°é¢æˆ–æ ‡é¢˜ selector: {selector}')
          break
      if not img_link:
        raise Exception('æ— æ³•æ‰¾åˆ°ç¬”è®°å°é¢æˆ–æ ‡é¢˜')

      # await img_link.evaluate('e => e.click()')  # ç›´æ¥ä½¿ç”¨ JSï¼Œé˜²æ­¢å°çº¢ä¹¦ä½¿ç”¨é®ç½©å±‚å±è”½ click äº‹ä»¶
      await img_link.click()

      # 2. ç­‰å¾…è¯¦æƒ…å¼¹çª—å‡ºç°
      note_container = self.page.locator('.note-container')
      await note_container.wait_for(state='visible', timeout=10000)

      logger.info(f'å½“å‰é¡µé¢url: {self.page.url}')

      current_url = self.page.url

      # 3. æå–ç¬”è®° ID
      note_id = extract_note_id(current_url)

      if not note_id:
        raise Exception('æ— æ³•æå–ç¬”è®°ID')

      url = normalize_url(current_url)

      logger.info(f'index: {index}, note_id: {note_id}, current_url: {current_url}')

      # TODO: è¿™é‡Œå¯ä»¥æ·»åŠ  note_idï¼ˆæˆ– current_urlï¼‰æ˜¯å¦å·²æŠ“å–é€»è¾‘ï¼Œå·²æŠ“å–è¿‡çš„ç¬”è®°å¯ä»¥è·³è¿‡

      # 4. æå–ç¬”è®°ä¿¡æ¯
      title = ''
      author = ''
      content = ''
      publish_time = None

      # è·å–æ ‡é¢˜
      title_element = note_container.locator('.note-content .title')
      title = await title_element.inner_text()
      logger.info(f'æå–åˆ°æ ‡é¢˜: {title}')

      # è·å–å›¾ç‰‡URLså¹¶åˆ›å»ºImageInfoå¯¹è±¡
      image_urls = await self._extract_image_urls(note_container)
      images = []

      for i, img_url in enumerate(image_urls):
        image_info = ImageInfo(src=img_url, alt=f'{title} - å›¾ç‰‡{i + 1}' if title else f'å›¾ç‰‡{i + 1}')
        images.append(image_info)

      # ä¸‹è½½å›¾ç‰‡
      if self.site.download_images and images:
        await self._download_images(note_id, images)

      # è·å–ä½œè€…
      author_element = note_container.locator('.author-container .username')
      author = await author_element.inner_text()
      logger.info(f'è·å–åˆ°ä½œè€…ï¼š{author}')

      # è·å–ç¬”è®°å†…å®¹
      content_element = note_container.locator('.note-content .desc')
      content = await content_element.inner_html()

      # è·å–å‘å¸ƒæ—¶é—´
      try:
        publish_time_element = note_container.locator('.note-content .date')
        publish_time_text = await publish_time_element.inner_text()
        publish_time = parse_publish_time(publish_time_text)
      except Exception as e:
        logger.warning(f'æ— æ³•è·å–å‘å¸ƒæ—¶é—´ä¿¡æ¯: {e}')
        publish_time = None

      logger.info(f'å‘å¸ƒæ—¶é—´ï¼š{publish_time}')

      # ç”Ÿæˆmarkdownå†…å®¹
      text = await content_element.locator('.note-text span').nth(0).inner_html()
      markdown = self._generate_markdown_from_html(text)

      # åˆ›å»ºCrawlPageå¯¹è±¡
      page_data = CrawlPageBase(
        id=note_id,
        url=url,
        crawl_url=current_url,
        title=title,
        author=author,
        publish_time=publish_time,
        content=content,
        markdown=markdown,
        images=images,
      )

      logger.info(f'æˆåŠŸæå–ç¬”è®°: {page_data.title}')
      return page_data

    except Exception:
      return None

  async def _extract_image_urls(self, note_container: Locator) -> list[str]:
    """æå–ç¬”è®°ä¸­çš„æ‰€æœ‰å›¾ç‰‡URL"""
    image_urls = []

    try:
      # å®šä½å›¾ç‰‡è½®æ’­å®¹å™¨
      swiper_wrapper = note_container.locator('.swiper-wrapper')
      await self.page.wait_for_timeout(1000)  # ç­‰å¾… swiper å…ƒç´ åŠ è½½

      # è·å–æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
      img_elements = await swiper_wrapper.locator('div[data-swiper-slide-index]:not(.swiper-slide-duplicate) img').all()
      img_count = len(img_elements)

      logger.info(f'æ‰¾åˆ°å›¾ç‰‡ä¸ªæ•°: {img_count}')

      for i in range(img_count):
        try:
          img_element = img_elements[i]
          # if not await img_element.is_visible():  # åªè·å–å¯è§çš„å›¾ç‰‡
          # continue
          src = await img_element.get_attribute('src')
          if not src:
            continue
          if src in image_urls:  # å»é‡
            logger.info(f'ç¬¬ {i} å¼ å›¾ç‰‡URLå·²å­˜åœ¨: {src}, è·³è¿‡')
            continue
          image_urls.append(src)
        except Exception as e:
          logger.warning(f'è·å–ç¬¬ {i} å¼ å›¾ç‰‡URLå¤±è´¥: {e}')
          continue

      logger.info(f'æˆåŠŸè·å–å›¾ç‰‡URLä¸ªæ•°: {len(image_urls)}')

    except Exception as e:
      logger.error(f'æå–å›¾ç‰‡URLså¤±è´¥: {e}')

    return image_urls

  def _generate_markdown_from_html(self, html_content: str) -> str:
    """ä»HTMLå†…å®¹ç”ŸæˆMarkdown"""
    try:
      # 1) å®ä¾‹åŒ–ç”Ÿæˆå™¨ï¼ˆå¯ä¼ è¿‡æ»¤å™¨ / å…¶å®ƒé€‰é¡¹ï¼Œåé¢è¯¦è¿°ï¼‰
      md_gen = DefaultMarkdownGenerator()

      # 2) ç”Ÿæˆ Markdown
      parsed_url = urlparse(self.page.url) if self.page else None
      base_url = f'{parsed_url.scheme}://{parsed_url.netloc}' if parsed_url else 'https://xiaohongshu.com'
      res = md_gen.generate_markdown(
        input_html=html_content,
        base_url=base_url,
        options={
          'ignore_links': True,
          'ignore_images': True,
          'ignore_emphasis': True,
        },  # å½“å‰ç”¨äº LLM ä¸éœ€è¦é“¾æ¥å’Œå›¾ç‰‡ï¼Œæ‰€ä»¥å¿½ç•¥
      )
      markdown = res.fit_markdown or res.raw_markdown or ''
      return markdown
    except Exception as e:
      logger.warning(f'HTMLè½¬Markdownå¤±è´¥: {e}')
      return html_content  # é™çº§è¿”å›HTMLå†…å®¹

  async def _download_images(self, note_id: str, images: list[ImageInfo]) -> None:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°å¹¶æ›´æ–°ImageInfoå¯¹è±¡"""
    logger.info(f'[å°çº¢ä¹¦] crawl_site.output_dir = {self.site.output_dir}')
    base_dir = os.path.join(self.site.output_dir, note_id)
    logger.info(f'\nä¸‹è½½å›¾ç‰‡åˆ°ç›®å½•: {base_dir}\n')
    os.makedirs(base_dir, exist_ok=True)
    # context = self.page.context
    # cookies = await context.cookies()

    async with httpx.AsyncClient(
      headers=self.site.extra_http_headers,
      timeout=httpx.Timeout(self.site.request_timeout or 30),
      # cookies=[cookie_to_dict(c) for c in cookies],
    ) as client:
      for i, image_info in enumerate(images):
        try:
          # ç”Ÿæˆæ–‡ä»¶å
          filename = f'{note_id}_{i + 1}.{self.site.image_format}'
          filepath = os.path.join(base_dir, filename)

          # ä¸‹è½½å›¾ç‰‡
          await self._download_single_image(client, image_info.src, filepath)
          # æ›´æ–°ImageInfoå¯¹è±¡
          image_info.local_path = filepath
          image_info.filename = filename
          logger.debug(f'å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {filename}')
          await asyncio.sleep(randint(50, 200) / 1000)

        except Exception:
          logger.error(f'ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {filename}', exc_info=True)
          continue

  async def _download_single_image(self, client: httpx.AsyncClient, img_url: str, filepath: str):
    """ä¸‹è½½å•å¼ å›¾ç‰‡"""
    response = await client.get(img_url)
    if response.status_code != 200:
      raise Exception(f'å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}')

    content = response.content

    # æ£€æŸ¥å›¾ç‰‡å¤§å°é™åˆ¶
    if self.site.max_image_size is not None and len(content) > self.site.max_image_size:
      raise Exception(f'å›¾ç‰‡è¶…å‡ºå¤§å°é™åˆ¶({len(content)} > {self.site.max_image_size}å­—èŠ‚)')

    async with aiofiles.open(filepath, 'wb') as f:
      await f.write(content)

  async def save_note(self, crawl_page: CrawlPageBase):
    """ä¿å­˜ç¬”è®°æ•°æ®"""
    filename_base = crawl_page.id or 'unknown'
    base_dir = os.path.join(self.site.output_dir, filename_base)
    os.makedirs(base_dir, exist_ok=True)

    crawl_page.clean_data()

    if self.site.save_as_db:
      # TODO: æ·»åŠ ä¿å­˜ç¬”è®°åˆ°æ•°æ®åº“æˆ–å…¶å®ƒæŒä¹…åŒ–å­˜å‚¨é€»è¾‘
      pass

    # ä¿å­˜ä¸ºJSON
    if self.site.save_as_json:
      json_path = os.path.join(base_dir, f'{filename_base}.json')
      async with aiofiles.open(json_path, 'w', encoding='utf-8') as f:
        await f.write(crawl_page.model_dump_json(indent=2))
      logger.info(f'JSONä¿å­˜æˆåŠŸ: {json_path}')

    # ä¿å­˜ä¸ºMarkdown
    if self.site.save_as_markdown:
      md_path = os.path.join(base_dir, f'{filename_base}.md')
      async with aiofiles.open(md_path, 'w', encoding='utf-8') as f:
        await f.write(crawl_page.markdown)
      logger.info(f'Markdownä¿å­˜æˆåŠŸ: {md_path}')
